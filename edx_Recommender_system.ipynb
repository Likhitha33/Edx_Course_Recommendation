{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Recommender system of edx course data\n",
        "\n",
        "\n",
        "**Content Based Filtering**\n",
        "\n",
        "\n",
        "**What we would like to achieve in this notebook**\n",
        "\n",
        "\n",
        "1.1 | Problem Formulation/Statement\n",
        "With the world becoming digital, any new skill can be acquired with just a click. However, many of us still needs a dedicated curriculum in order to excel in a specific topic.\n",
        "\n",
        "This is where e-learning platforms comes handy and EdX is one of such massive open online course (MOOC) providers.\n",
        "\n",
        "So we've found a course we like, and went through the course, so what next?\n",
        "\n",
        "With the availability of so many online courses, it may be take some effort and time to look through all available courses.\n",
        "We can utilise a recommendation system to give some tips on what course the user might like to go though next\n",
        "Whilst there are quite a number approaches to recommendation systems, well utilise an approach which requires NLP\n",
        "\n",
        "1.2 | Recommendation system\n",
        "\n",
        "GOALS\n",
        "\n",
        "The purpose of our recommendation system is to inform a user about possible courses they make like, based on a course they liked.\n",
        "\n",
        "\n",
        "METHOD\n",
        "\n",
        "We will utilise scrapped course description data (our corpus), well convert each document into vector form using (bow,embeddings), then calculate the consine similarity, from which we will be able to extract courses which are most similar.\n",
        "\n",
        "\n",
        "1.3 | The Dataset\n",
        "\n",
        "This dataset is scraped off the publicly available information on the EdX website.\n",
        "This dataset consists of 720 rows and 6 columns namely Name of the Course, Name of the University, Difficulty Level, Course URL, short summary about the course and course description\n",
        "What is edX?\n",
        "\n",
        "edX online courses are self-paced, interactive courses offered by leading universities and organizations around the world. These courses provide learners with a range of topics to explore and learn from, including computer science, business, health, engineering, humanities, and more. With edX courses, learners can gain valuable skills and knowledge in an engaging and convenient way.\n",
        "\n",
        "Image\n",
        "\n",
        "\n",
        "1.4 | Notebook Goals\n",
        "\n",
        "Two subgoals are of interest:\n",
        "\n",
        "EDA study | Analyse an draw conclusions based on the courses that are available\n",
        "\n",
        "Course Recommendation system | Create a course recommendation based on a specified course.\n",
        "\n",
        "\n",
        "\n",
        "2 | idX DATASET\n",
        "\n",
        "WHAT WE WILL DO IN THIS SECTION\n",
        "\n",
        "We'll read the data EdX.csv\n",
        "\n",
        "Lower the register of column names\n",
        "\n",
        "Show for one course the name, about & description"
      ],
      "metadata": {
        "id": "CL0A0a2LBXFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Import"
      ],
      "metadata": {
        "id": "DfauBAn_wdp6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhCDR2kQiYY6"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add the path of edx csv file\n",
        "data1=pd.read_csv('**path to edx csv file**')\n",
        "data=data1.copy()\n",
        "data.head()"
      ],
      "metadata": {
        "id": "F0lQUESkBOsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for **corpus** we are utilising name, about and course description"
      ],
      "metadata": {
        "id": "JRsC_4Gs1PdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "jTZTD6iC4Dcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "hpsuKNTvBOpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "STvdrZyYBOnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.reset_index(drop = True, inplace = True)"
      ],
      "metadata": {
        "id": "W3NM2MNLBOj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "-qKuh5WKBOhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "zt4RHa-OBOeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "roWOw8n24euB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['University'].value_counts()"
      ],
      "metadata": {
        "id": "1HZwa8oiBObb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Difficulty Level'].value_counts()"
      ],
      "metadata": {
        "id": "PE8ZvmFQ4mTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Link'].nunique()"
      ],
      "metadata": {
        "id": "jHjUlqMl4sEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "data['About'].nunique()"
      ],
      "metadata": {
        "id": "LMPx-e-U4zos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['About'].value_counts()"
      ],
      "metadata": {
        "id": "iOiY8K2w45Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Name'].value_counts()"
      ],
      "metadata": {
        "id": "IdsiAf_74_tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "qmOAQlmMu6Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=data[['University','Difficulty Level']]\n",
        "df"
      ],
      "metadata": {
        "id": "ViV_FfP3u6Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import numpy as np\n",
        "from google.colab import autoviz\n",
        "\n",
        "def categorical_histogram(df, colname, figscale=1, mpl_palette_name='Dark2'):\n",
        "  from matplotlib import pyplot as plt\n",
        "  import seaborn as sns\n",
        "  df.groupby(colname).size().plot(kind='barh', color=sns.palettes.mpl_palette(mpl_palette_name), figsize=(8*figscale, 4.8*figscale))\n",
        "  plt.gca().spines[['top', 'right',]].set_visible(False)\n",
        "  return autoviz.MplChart.from_current_mpl_state()\n",
        "\n",
        "chart = categorical_histogram(df, *['Difficulty Level'], **{})\n",
        "chart"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "72v5xr2Dvhwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pie(x=df['Difficulty Level'].value_counts(),labels=df['Difficulty Level'].value_counts().index,autopct='%0.2f%%',data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HiZDMK0PPxNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 6))\n",
        "# Group the data by university and difficulty level\n",
        "grouped_data = df.groupby(['University', 'Difficulty Level']).size().unstack(fill_value=0)\n",
        "\n",
        "# Filter universities that have all three difficulty levels\n",
        "universities_with_all_levels = grouped_data[\n",
        "    (grouped_data['Beginner'] > 0) &\n",
        "    (grouped_data['Intermediate'] > 0) &\n",
        "    (grouped_data['Advanced'] > 0)\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Define colors for each difficulty level\n",
        "color_map = {\n",
        "    'Beginner': 'skyblue',\n",
        "    'Intermediate': 'gold',\n",
        "    'Advanced': 'lightgreen'\n",
        "}\n",
        "\n",
        "ax = universities_with_all_levels.plot(kind='bar',width=0.9,stacked=True,\n",
        "                                       color=[color_map[level] for level in universities_with_all_levels.columns])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "shortened_labels = [label if len(label) <= 15 else label[:17] + '...' for label in universities_with_all_levels.index]\n",
        "\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    if height > 0:\n",
        "        ax.annotate(str(int(height)), (p.get_x() + p.get_width() / 2., height), ha='center', va='center', color='black', fontweight='normal', fontsize=8)\n",
        "\n",
        "\n",
        "\n",
        "ax.set_xticks(range(len(universities_with_all_levels)))\n",
        "ax.set_xticklabels(shortened_labels, rotation='vertical')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('University')\n",
        "plt.ylabel('Number of Courses')\n",
        "plt.title('Courses by Difficulty Level for Universities with All Levels')\n",
        "plt.xticks(rotation='vertical')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Display the legend\n",
        "ax.legend(title='Difficulty level', labels=color_map.keys())\n",
        "\n",
        "# Display the chart\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZW1emqO6PxKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQq-mZDBPxGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['About'].head()"
      ],
      "metadata": {
        "id": "KL0ClDKNu58i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# n-gram of course description"
      ],
      "metadata": {
        "id": "CwfjbEJvwa3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "J7WLylBzz0Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "REhCQ6Ke0D5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "stopwords_en = stopwords.words('english')\n",
        "lemma=WordNetLemmatizer()\n",
        "\n",
        "def cleaning(text):\n",
        "  text=re.sub(\"[^a-zA-Z1-9]\",\" \",text) # remove punctuation marks\n",
        "  text=text.lower()\n",
        "  tokens=word_tokenize(text)\n",
        "  cleaned_list=[]\n",
        "  for token in tokens:\n",
        "    if token not in stopwords_en:\n",
        "      cleaned_list.append(lemma.lemmatize(token))\n",
        "  return  \" \".join(cleaned_list)\n",
        "\n",
        "df1=data['Course Description'].apply(cleaning)"
      ],
      "metadata": {
        "id": "A-FD6hgXu55f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "xpLADlwO5MzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "ngrams = {'unigrams':[],'bigrams':[],'trigrams':[]}\n",
        "\n",
        "for document in df1:\n",
        "  doc=nlp_en(document)\n",
        "  tokens=[token.text for token in doc]\n",
        "\n",
        "  def ngrams1(tokens,n):\n",
        "    list_ngrams=[' '.join(i) for i in [tokens[i:i+n]  for i in range(len(tokens)-n+1)]]\n",
        "    return list_ngrams\n",
        "  ngrams['unigrams'].extend(ngrams1(tokens,1))\n",
        "  ngrams['bigrams'].extend(ngrams1(tokens,2))\n",
        "  ngrams['trigrams'].extend(ngrams1(tokens,3))\n",
        "\n",
        "print(ngrams['unigrams'][0:3])\n",
        "print('unigrams : ',len(ngrams['unigrams']))\n",
        "print(ngrams['bigrams'][0:3])\n",
        "print('bigrams : ',len(ngrams['bigrams']))\n",
        "print(ngrams['trigrams'][0:3])\n",
        "print('trigrams : ',len(ngrams['trigrams']))\n"
      ],
      "metadata": {
        "id": "UzxiWgpd0MiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrams1(tokens,1)"
      ],
      "metadata": {
        "id": "_yz7aProsInR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_counter(counter,top,name):\n",
        "    labels, values = zip(*counter.items())\n",
        "    fig = px.bar(pd.Series(values,index=labels,name=name).sort_values(ascending=False)[:top],\n",
        "                 template='plotly_white',orientation='h')\n",
        "    fig.show()\n",
        "\n",
        "plot_counter(Counter(ngrams['unigrams']),10,'unigram')\n",
        "plot_counter(Counter(ngrams['bigrams']),10,'unigram')\n",
        "plot_counter(Counter(ngrams['trigrams']),10,'unigram')\n"
      ],
      "metadata": {
        "id": "Iw3A3nTMFLQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Natural Language Processing"
      ],
      "metadata": {
        "id": "7ECFzZlkKq3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Remove irrelovant columns in our data that won't be utilised in this study\n",
        "*   Create a new columns text, which will be used in our analysis\n",
        "\n",
        "\n",
        "*   Do some text cleaning & stemming of the text column data\n",
        "*   Prepare the data for both TF-IDF & Word2Vec, which require slightly different inputs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bcnxVYOaKxt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Drop irrelavant columns**"
      ],
      "metadata": {
        "id": "zOL7C4v7K-KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "2f-sbTdCMt4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(columns=['University','Difficulty Level'],axis=1,inplace=True)\n",
        "df2=data.copy()\n",
        "df2.head()\n"
      ],
      "metadata": {
        "id": "tvFvWOXoJqIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a documents which will be comprised of the course name, about & course description\n",
        "\n",
        "We will be utilising this as our corpus data we will feed into TF-IDF & Word2Vec models\n",
        "\n",
        "data['text'] will be our corpus"
      ],
      "metadata": {
        "id": "tXaDOcoLNPPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['text']= df2['Name']+' '+df2['About']+' '+df2['Course Description']\n",
        "data.head()"
      ],
      "metadata": {
        "id": "8EkuYMMiNCcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = data[['Name','About','Course Description','text']]\n",
        "text_data.to_csv('text_data.csv',index=False)"
      ],
      "metadata": {
        "id": "xft4ZVCFNjlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'][0]"
      ],
      "metadata": {
        "id": "mO2d8uBJl49G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning / Stemming"
      ],
      "metadata": {
        "id": "S4SzfUu-OS6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer"
      ],
      "metadata": {
        "id": "zDRyJy5rNxT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_stopwords = stopwords.words(\"english\") # stop words\n",
        "lemma = WordNetLemmatizer() # lemmatiser\n",
        "\n",
        "# define a function for preprocessing\n",
        "def clean(text):\n",
        "    text = re.sub(\"[^A-Za-z1-9 ]\", \"\", text) #removes punctuation marks\n",
        "    text = text.lower()\n",
        "     #changes to lower case\n",
        "    tokens = word_tokenize(text) #tokenize the text\n",
        "    clean_list = []\n",
        "    for token in tokens:\n",
        "        if token not in en_stopwords: #removes stopwords\n",
        "            clean_list.append(lemma.lemmatize(token)) #lemmatizing and appends to clean_list\n",
        "    return \" \".join(clean_list)# joins the tokens\n",
        "\n",
        "# applying the \"clean\" function on the text column\n",
        "data.text = data.text.apply(clean)\n",
        "\n",
        "data.text"
      ],
      "metadata": {
        "id": "CDI4pwN3Owni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing, returns list instead\n",
        "def clean_for_word2vec(text):\n",
        "\n",
        "    text = re.sub(\"[^A-Za-z1-9 ]\", \"\", text) #removes punctuation marks\n",
        "    text = text.lower() #changes to lower case\n",
        "    tokens = word_tokenize(text) #tokenize the text\n",
        "    clean_list = []\n",
        "    for token in tokens:\n",
        "        if token not in en_stopwords: #removes stopwords\n",
        "            clean_list.append(lemma.lemmatize(token)) #lemmatizing and appends to clean_list\n",
        "    return clean_list\n",
        "\n",
        "#cleaning the documents\n",
        "corpus_cleaned = data.text.apply(clean_for_word2vec)\n",
        "lst_corpus = corpus_cleaned.tolist()"
      ],
      "metadata": {
        "id": "cR4cZaaQwkgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus_cleaned[0])"
      ],
      "metadata": {
        "id": "-cysb3Fr5OHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus_cleaned)"
      ],
      "metadata": {
        "id": "OHtd0mcPAYkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for words in data['text']:\n",
        "    corpus.append(words.split())\n",
        "\n",
        "len(f'corpus length: {corpus}')"
      ],
      "metadata": {
        "id": "Y9fME-UXzZBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'][0]"
      ],
      "metadata": {
        "id": "DGHnHsRC5zof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "vNronF695G3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "course_list={}\n",
        "names1={}\n",
        "tags1={}\n",
        "for i in range(len(data['text'])):\n",
        "  names1[i]=data['Name'][i]\n",
        "  tags1[i]=data['text'][i]\n",
        "\n",
        "course_list['course_name']=names1\n",
        "course_list['tags']=tags1\n",
        "course_list"
      ],
      "metadata": {
        "id": "AncU7qNKpV4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#course_lists=course_list.to_pickle('course_list_edx.pkl')\n",
        "import pickle\n",
        "pickle.dump(course_list, open(\"course_list_edx.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "E0dwyJHvvCGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COURSE RECOMMENDATIONS"
      ],
      "metadata": {
        "id": "OynvnNB7zd_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our approach to providing recommendations is based on cosine similarity of input vectors\n",
        "\n",
        "The first approach we can utilise to generate vectors for each course is by utilising Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "The second approach we can utilise to generate vectors for each course is by utilising Embedding Vectors"
      ],
      "metadata": {
        "id": "uG0-dLR4ziZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_names=list(data['Name'])\n",
        "list_names[:5]"
      ],
      "metadata": {
        "id": "0j1VRLCfzbyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text']"
      ],
      "metadata": {
        "id": "vCUnrWKWBFT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df=pd.DataFrame(list_names,columns=['course_name'])\n",
        "new_df['tags']=data['text']\n",
        "new_df['Link']=data['Link']\n",
        "new_df"
      ],
      "metadata": {
        "id": "CwMo22BFdpQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "courses=new_df.to_pickle('courses.pkl')"
      ],
      "metadata": {
        "id": "8JFFkUsahtwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GENERATION OF VECTOR REPRESENTATION OF TEXT**\n",
        "\n",
        "* TF-IDF was described in notebook nlp | Natural Language Processing Reference\n",
        "\n",
        "* test_matrix, is input into our recommendation generation function Recommendation_Cosine_similarity"
      ],
      "metadata": {
        "id": "k0Z4oo7P0IUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "vectoriser=TfidfVectorizer()\n",
        "test_matrix = vectoriser.fit_transform(data['text']).toarray()\n",
        "\n",
        "test_matrix.shape"
      ],
      "metadata": {
        "id": "ER6NKPdlz9B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(test_matrix)"
      ],
      "metadata": {
        "id": "xHKBBDUmBj4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectoriser.vocabulary_)"
      ],
      "metadata": {
        "id": "R4PwZ7GW3lZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATION OF VECTOR REPRESENTATION OF TEXT\n",
        "\n",
        "Example recommendation for: MathTrackX: Differential Calculus\n",
        "\n",
        "1 MathTrackX: Differential Calculus\n",
        "\n",
        "2 MathTrackX: Integral Calculus\n",
        "\n",
        "3 MathTrackX: Statistics\n",
        "\n",
        "4 MathTrackX: Polynomials, Functions and Graphs\n",
        "\n",
        "5 MathTrackX: Probability\n",
        "\n",
        "6 MathTrackX: Special Functions"
      ],
      "metadata": {
        "id": "ht71AsCt3uQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity=cosine_similarity(test_matrix)"
      ],
      "metadata": {
        "id": "FAapuYimfs9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pickle.dump(similarity, open(\"similarity_edx.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "_igqQ2RpwcRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend(course):\n",
        "    course_index = new_df[new_df['course_name'] == course].index[0]\n",
        "\n",
        "    distances = similarity[course_index]\n",
        "    course_list = sorted(list(enumerate(distances)),reverse=True, key=lambda x:x[1])[1:7]\n",
        "\n",
        "    for i in course_list:\n",
        "        print(new_df.iloc[i[0]].course_name,\":\")\n",
        "        print(data1.iloc[i[0]].Link)\n",
        "recommend('MathTrackX: Differential Calculus')"
      ],
      "metadata": {
        "id": "5uHPwyGydGFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Recommendation_wth_Cos_similarity(matrix,name):\n",
        "\n",
        "  row_ind=list_names.index(name)\n",
        "  similarity=cosine_similarity(matrix)\n",
        "\n",
        "  #getting the course with highest cosine similarity\n",
        "  similar_courses = list(enumerate(similarity[row_ind]))\n",
        "  sorted_courses=sorted(similar_courses, key=lambda x:x[1] , reverse=True)[:6]\n",
        "\n",
        "  print(f'Recommended course for {name} \\n')\n",
        "\n",
        "  i=0\n",
        "  for course in sorted_courses:\n",
        "    course_des=data[ data.index == course[0]]['Name']\n",
        "    recommendation = print(f'{i+1} {course_des}')\n",
        "    i=i+1\n",
        "  return recommendation\n",
        "\n",
        "Recommendation_wth_Cos_similarity(test_matrix,'MathTrackX: Statistics')"
      ],
      "metadata": {
        "id": "jVg42TXU4NQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pg1ChojTFChF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}